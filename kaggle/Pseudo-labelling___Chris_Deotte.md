```https://www.kaggle.com/competitions/playground-series-s6e1/discussion/666888```
# Improve CV and LB with Pseudo Labels

One trick I use in many Kaggle playground competitions is to boost CV and LB with pseudo labels. This advanced technique is often overlooked but can help gain some extra ranks on the leaderboard.

How to Pseudo Label
To pseudo label in Kaggle's playground competition, we can use this simple technique. Inside each KFold for loop, we train each model twice (or we load saved oof_preds and test_preds from previously trained models). Here is example code
```python
kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)

for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):

    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]

    ##################
    ### TRAIN MODEL ONE
    model = MODEL_ONE()
    model.fit(
        X_train, y_train,
        X_val, y_val,
    )
    oof_preds = model.predict(X_val)
    test_preds = model.predict(X_test)

    ##################
    ### PSEUDO LABEL
    X_train2 = pd.concat([X_train,X_val,X_test],axis=0)
    y_train2 = np.concatenate([y_train,oof_preds,test_preds],axis=0)

    ##################
    ### TRAIN MODEL TWO w/ PSEUDO
    model = MODEL_TWO()
    model.fit(
        X_train2,
        y_train2,
    )
    oof_preds = model.predict(X_val)
    test_preds = model.predict(X_test)
```
# Evaluate Improvement
It is easy to evaluate the improvement. If we use the same model each time (i.e. XGB both times). Then we can compute the OOF CV score of model 1 and compare it to the OOF CV score of model 2. The difference is our improvement.

# Why does Pseudo Labeling Work?
It is confusing why pseudo labeling helps. If we train XGB twice, how can XGB teach itself? What benefit do we get using fake targets generated by XGB to help train XGB?

The answer is more features not more targets. When we apply fake targets to unlabeled data and then train with the data, we don't benefit from the fake targets. We benefit from the new real features. A model must understand the relationship between features to make more accurate predictions.

It is easiest to understand with transformer NN models. Transformers need to learn attention which is the relationship between features. When we train with additional pseudo labeled data, we give the transformer more real features to learn better attention which improves predicting targets. Pseudo labeling works for all models. For another explanation, see diagrams of how pseudo labeling improves the ML model QDA here

# Knowledge Distillation
If we use a different model for ONE and TWO, then we also benefit from knowledge distillation. In additional to the benefits from pseudo labels giving us more train data, we also transfer the intelligence of model one into model two. For example, model one can be TabM and model two can be XGB. Then we will teach XGB from the knowledge of TabM. This helps model two reach a level of intelligence that may have been impossible to accomplish without knowledge distillation.

Starter Notebook
I publish a starter notebook here https://www.kaggle.com/code/cdeotte/xgb-w-tabm-pseudo-labels-cv-8-60
